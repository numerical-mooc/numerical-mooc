{
 "metadata": {
  "name": "",
  "signature": "sha256:b8328f3fff343a0860b3ab5524ee3708b4283c93692ab27e56416331baacd4fb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Content under Creative Commons Attribution license CC-BY 4.0, code under MIT license (c)2014 L.A. Barba, C.D. Cooper, G.F. Forsyth.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Iterate This!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In [Lesson 1](./2D_Laplace_Equation.ipynb) and [Lesson 2](./2D_Poisson_Equation.ipynb) of this module we used the Jacobi method to iteratively solve for solutions to elliptic PDEs.  \n",
      "\n",
      "And it worked, so why are we still talking about it?  Because it's slow.  Very, very slow.  It might not have seemed that way in the first two notebooks because our domains were quite tiny, but consider this:  for a domain with $nx = ny = 128$, the Jacobi method will require more than *21000* iterations before the residual dips below $10^{-8}$.\n",
      "\n",
      "Using one core of an Intel i7 1.9 GHz processor, that takes around 4.5 seconds.  Now consider this:  an incompressible Navier Stokes solver has to ensure that the pressure field is divergence-free at every timestep.  One of the most common ways to ensure that the pressure field is divergence-free is to *relax* the pressure field using *iterative methods*!  \n",
      "\n",
      "In fact, the pressure condition is responsible for the majority of the computational expense of a Navier Stokes solver.  And with the current set up, each timestep could require 4.5 seconds of CPU time just to satisfy the pressure constraints!  We'll grow old and die before we ever get cavity flow working!\n",
      "\n",
      "There has to be a better way.  And, of course, there is.  There are several!\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll be using the same example problem that we covered in [Lesson 1](./2D_Laplace_Equation.ipynb), with boundary conditions\n",
      "\n",
      "\\begin{equation}\n",
      "  \\begin{gathered}\n",
      "p=0 \\text{ at } x=0\\\\\n",
      "\\frac{\\partial p}{\\partial x} = 0 \\text{ at } x = L\\\\\n",
      "p = 0 \\text{ at }y = 0 \\\\\n",
      "p = \\sin \\left(  \\frac{\\frac{3}{2}\\pi x}{L} \\right) \\text{ at } y = H\n",
      "  \\end{gathered}\n",
      "\\end{equation}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "from matplotlib import pyplot\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib import cm\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead of copying and pasting cells that we wrote in [Lesson 1](), we have again created a 'helper' file that you can import some useful functions from.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from laplace_helper import p_analytical, plot2D, L2_rel_error"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have the `p_analytical`, `plot2D`, and `L2_rel_error` functions in our namespace.  If you can't remember how they work, just use `help()` and take advantage of the docstrings.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Test Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are going to use larger grid dimensions in this notebook to better illustrate the speed increases available with different iterative methods.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx = 128\n",
      "ny = 128\n",
      "\n",
      "L = 5.\n",
      "H = 5.\n",
      "\n",
      "x = numpy.linspace(0,L,nx)\n",
      "y = numpy.linspace(0,H,ny)\n",
      "\n",
      "dx = L/(nx-1)\n",
      "dy = H/(ny-1)\n",
      "\n",
      "p0 = numpy.zeros((ny, nx))\n",
      "\n",
      "p0[-1,:] = numpy.sin(1.5*numpy.pi*x/x[-1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First things first -- we said above that the Jacobi method takes more than 21000 iterations before it satisfies the target residual of $10^{-8}$, but let's verify that.  \n",
      "\n",
      "We can also time how long it takes to complete those iterations using the `time` module.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def laplace2d(p, y, dx, dy, l2_target):\n",
      "    '''Solves the diffusion equation with forward-time, centered scheme\n",
      "    \n",
      "    Parameters:\n",
      "    ----------\n",
      "    p: 2D array of float\n",
      "        Initial potential distribution\n",
      "    y: array of float\n",
      "        Nodal coordinates in y\n",
      "    dx: float\n",
      "        Mesh size\n",
      "    dy: float\n",
      "        Mesh size\n",
      "    l2_target: float\n",
      "        Error target\n",
      "        \n",
      "    Returns:\n",
      "    -------\n",
      "    p: 2D array of float\n",
      "        Potential distribution after relaxation\n",
      "    '''\n",
      "    \n",
      "    l2norm = 1\n",
      "    pn = numpy.empty_like(p)\n",
      "    iterations = 0\n",
      "\n",
      "    while l2norm > l2_target:\n",
      "        pn = p.copy()\n",
      "        p[1:-1,1:-1] = .25 * (pn[1:-1,2:] + pn[1:-1,:-2] +\\\n",
      "                              pn[2:,1:-1] + pn[:-2,1:-1])\n",
      "        \n",
      "        ##Neumann B.C. along x = L\n",
      "        p[1:-1,-1] = .25 * (2*p[1:-1,-2] + p[2:,-1] + p[:-2, -1])\n",
      "        \n",
      "        l2norm = numpy.sqrt(numpy.sum((p - pn)**2)/numpy.sum(pn**2))\n",
      "        iterations += 1\n",
      "     \n",
      "    return p, iterations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "p = p0.copy()\n",
      "eps = 1e-8\n",
      "p, iterations = laplace2d(p, y, dx, dy, eps)\n",
      "t2 = time.time()\n",
      "\n",
      "print \"Jacobi method took {:.4f} seconds over {} iterations at tolerance {}\".\\\n",
      "        format(t2-t1, iterations, eps)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Would we lie to you?  21268 iterations before the relative L^2-norm dips below $10^-8$.  That's a large number of iterations, but is it even accurate?  Let's compare it to the analytical solution and check.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pan = p_analytical(x,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L2_rel_error(p,pan)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, that's a pretty small error.  Let's focus on speeding up the process.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gauss-Seidel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You will recall from [Lesson 1](./2D_Laplace_Equation.ipynb) that a single Jacobi iteration is written as:\n",
      "\n",
      "\\begin{equation}\n",
      "p^{n+1}_{i,j} = \\frac{1}{4} \\left(p^{n}_{i,j-1} + p^n_{i,j+1} + p^{n}_{i-1,j} + p^n_{i+1,j} \\right)\n",
      "\\end{equation}\n",
      "\n",
      "The Gauss-Seidel method is a simple tweak to this idea -- use updated values as soon as they are available.  \n",
      "\n",
      "If you imagine that we progress through an array in the following order:\n",
      "\n",
      "<img src=\"./figures/solvepath.svg\" width=350>\n",
      "\n",
      "\n",
      "Then you can see that the values $p^{n+1}_{i-1,j}$ and $p^{n+1}_{i,j-1}$ can be used to calculate $p^{n+1}_{i,j}$, so the iteration formula will now read as:\n",
      "\n",
      "\\begin{equation}\n",
      "p^{n+1}_{i,j} = \\frac{1}{4} \\left(p^{n+1}_{i,j-1} + p^n_{i,j+1} + p^{n+1}_{i-1,j} + p^n_{i+1,j} \\right)\n",
      "\\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But there's a problem.  You can't use NumPy's array operations to evaluate that.  Since Gauss-Seidel requires using values immediately after they're updated, we have to abandon our beloved array operations and return to nested `for` loops.  \n",
      "\n",
      "That's not ideal, but if it saves us a bunch of time, then we can manage.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def laplace2d_gauss_seidel(p, y, dx, dy, nx, ny, eps):\n",
      "    \n",
      "    iterations = 0\n",
      "    error = 2*eps\n",
      "    \n",
      "    while error > eps:\n",
      "        pn = p.copy()\n",
      "        error = 0.0\n",
      "        for j in range(1,ny-1):\n",
      "            for i in range(1,nx-1):\n",
      "                p[j,i] = .25 * (p[j,i-1] + p[j,i+1] + p[j-1,i] + p[j+1,i])\n",
      "                error += (p[j,i] - pn[j,i])**2\n",
      "        \n",
      "        #Neumann 2nd-order BC\n",
      "        for j in range(1,ny-1):\n",
      "            p[j,-1] = .25 * (2*p[j,-2] + p[j+1,-1] + p[j-1, -1])\n",
      "            \n",
      "        error = numpy.sqrt(error/numpy.sum(pn**2))\n",
      "        iterations += 1        \n",
      "        \n",
      "    return p, iterations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And then we would run this via:\n",
      "\n",
      "```Python\n",
      "p, iterations = laplace2d_gauss_seidel(p, y, dx, dy, nx, ny, 1e-8)\n",
      "```\n",
      "\n",
      "<br>\n",
      "But **don't do it**.  We did it so that you don't have to.  \n",
      "\n",
      "The Gauss-Seidel method required several thousand fewer iterations than the traditional Jacobi method for this example, but it took more than *7 minutes* to run.  \n",
      "\n",
      "And we were complaining about 4.9 seconds!?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "What happened?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you think back to the far off days when you first learned about array operations, you might recall that we discovered that NumPy array operations could drastically improve code performance compared with nested `for` loops.  NumPy operations are largely written in compiled C code and they are *much* faster than vanilla Python.  But the Jacobi method is old and while 4.9 seconds is much better than 7 minutes, it's still too slow.  What can we do?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Blitz it"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Enter `weave.blitz`\n",
      "\n",
      "`weave.blitz` takes an existing NumPy expression, converts it into C++ code and uses the C++ blitz++ library to compile it.  Then it repacks it into a Python function and it's ready for you to use.  \n",
      "\n",
      "But didn't we just explain that array operations don't work with Gauss-Seidel because we can't use immediately updated values?  Yes we did and that is true.  \n",
      "\n",
      "*Except*, when `weave.blitz` converts the code it doesn't use any temporary arrays to store a previous solution.  What does that mean for us?  It means that it will perform the array operation but in the underlying C++ code, it will use the most recently updated values as it evaluates our array.  \n",
      "\n",
      "In other words, it will implement Gauss-Seidel for us!\n",
      "\n",
      "Let's get to it.  First, we need to import the `weave` library from SciPy.  \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import weave"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We only make a few changes to our original Jacobi method code and they will seem a little strange.  \n",
      "\n",
      "Make the array operation for calculating $p^{n+1}_{i,j}$ into a string.  Then send that string to `weave.blitz` along with the keyword `check_size=0`.  \n",
      "\n",
      "The `check_size` keyword performs a few sanity checks if it's set equal to 1, but we can skip those in exchange for an extra speedup.  If you have bugs in your code, you can set it equal to 1 to help track them down.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def laplace2d_gauss_seidel_blitz(p, y, dx, dy, eps):\n",
      "    \n",
      "    iterations = 0\n",
      "    error = 2*eps\n",
      "    \n",
      "    while error > eps:\n",
      "        pn = p.copy()\n",
      "        error = 0.0\n",
      "        \n",
      "        expr = \"p[1:-1,1:-1] = \\\n",
      "                .25 * (p[1:-1,:-2] + p[1:-1,2:] + p[:-2,1:-1] + p[2:,1:-1])\"\n",
      "        \n",
      "        weave.blitz(expr, check_size=0)\n",
      "        \n",
      "        #Neumann 2nd-order BC\n",
      "        p[1:-1,-1] = .25 * (2*p[1:-1,-2] + p[2:,-1] + p[:-2, -1])\n",
      "        \n",
      "        error = numpy.sqrt(numpy.sum((p - pn)**2)/numpy.sum(pn**2))\n",
      "        iterations += 1\n",
      "        \n",
      "    return p, iterations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "p = p0.copy()\n",
      "eps = 1e-8\n",
      "p, iterations = laplace2d_gauss_seidel_blitz(p, y, dx, dy, eps)\n",
      "t2 = time.time()\n",
      "\n",
      "print \"Blitzed GS method took {:.4f} seconds over {} iterations at tolerance {}\".\\\n",
      "        format(t2-t1, iterations, eps)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nice!  We're almost under 3 seconds now with around 7000 fewer iterations!  Not bad for 2 extra lines of code.  Don't forget to make sure that the L2 error looks reasonable!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L2_rel_error(p,pan)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is definite progress.  Think we can do any better?  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Successive Over-Relaxation (SOR)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Successive over-relaxation is an extension of the Gauss-Seidel method.  We take the existing Gauss-Seidel method and use a linear combination of the previous and the current solution to accelerate convergence.  \n",
      "\n",
      "\\begin{equation}\n",
      "p^{n+1}_{i,j} = (1 - \\omega)p^n_{i,j} + \\frac{\\omega}{4} \\left(p^{n+1}_{i,j-1} + p^n_{i,j+1} + p^{n+1}_{i-1,j} + p^n_{i+1,j} \\right)\n",
      "\\end{equation}\n",
      "\n",
      "SOR iterations are only stable for $0 < \\omega < 2$.  \n",
      "\n",
      "If $\\omega < 1$, that is technically an \"under-relaxation\" and it will be slower than Gauss-Seidel.  \n",
      "\n",
      "If $\\omega > 1$, that's the over-relaxation and it should converge faster than Gauss-Seidel.  \n",
      "\n",
      "Also note that for $\\omega = 1$, the equation collapses into the Gauss-Seidel method.  \n",
      "\n",
      "Everything we did above using `weave.blitz` still applies here -- the only things we need to do are implement the array operation for $p^{n+1}_{i,j}$ and add $\\omega$ to the function inputs.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def laplace2d_SOR_blitz(p, y, dx, dy, eps, omega):\n",
      "    \n",
      "    iterations = 0\n",
      "    error = 2*eps\n",
      "    \n",
      "    while error > eps:\n",
      "        pn = p.copy()\n",
      "        error = 0.0\n",
      "        \n",
      "        expr = \"p[1:-1,1:-1] = (1 - omega) * p[1:-1,1:-1] +\\\n",
      "        .25 * omega * (p[1:-1,2:]+p[1:-1,:-2]+p[2:,1:-1]+p[:-2,1:-1])\"\n",
      "        \n",
      "        weave.blitz(expr, check_size=0)\n",
      "        \n",
      "        #Neumann 2nd-order BC\n",
      "        p[1:-1,-1] = .25 * (2*p[1:-1,-2] + p[2:,-1] + p[:-2, -1])\n",
      "        \n",
      "        error = numpy.sqrt(numpy.sum((p - pn)**2)/numpy.sum(pn**2))\n",
      "        \n",
      "        iterations += 1\n",
      "        \n",
      "        \n",
      "    return p, iterations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That wasn't too bad at all.  Let's try this out first with $\\omega = 1$ and make sure it matches the Gauss-Seidel results from above.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = p0.copy()\n",
      "t1 = time.time()\n",
      "eps = 1e-8\n",
      "omega = 1\n",
      "p, iterations = laplace2d_SOR_blitz(p, y, dx, dy, eps, omega)\n",
      "t2 = time.time()\n",
      "\n",
      "print \"Blitzed SOR method took {:.4f} seconds and {} iterations\\\n",
      " at tolerance {} with omega = {}\".format(t2 - t1, iterations, eps, omega)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The execution time is going to vary depending on what else the computer is up to at the moment, but we have the exact same number of iterations as Gauss-Seidel.  That's a good sign that things are working as expected.  \n",
      "\n",
      "Now let's try to over-relax the solution and see what happens.  To start, let's try $\\omega = 1.5$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = p0.copy()\n",
      "t1 = time.time()\n",
      "eps = 1e-8\n",
      "omega = 1.5\n",
      "p, iterations = laplace2d_SOR_blitz(p, y, dx, dy, eps, omega)\n",
      "t2 = time.time()\n",
      "\n",
      "print \"Blitzed SOR method took {:.4f} seconds and {} iterations\\\n",
      " at tolerance {} with omega = {}\".format(t2 - t1, iterations, eps, omega)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wow!  That really did the trick!  Under 1.5 seconds!  And we dropped from 15014 iterations down to 7406.  Now we're really cooking!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tuned SOR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We picked $\\omega=1.5$ somewhat arbitrarily.  Ideally, we would like to over-relax the solution as much as possible without introducing instability, as that will give us the fewest number of iterations.  \n",
      "\n",
      "For square domains, it turns out that the ideal factor $\\omega$ can be computed as a function of the number of nodes in one direction, e.g. `nx`.  \n",
      "\n",
      "\\begin{equation}\n",
      "\\omega \\approx \\frac{2}{1+\\frac{\\pi}{nx}}\n",
      "\\end{equation}\n",
      "\n",
      "This is not some arbitrary formula, but its derivation lies outside the scope of this class.  For now, let's try it out and see how it works.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = p0.copy()\n",
      "t1 = time.time()\n",
      "eps = 1e-8\n",
      "omega = 2./(1 + numpy.pi/nx)\n",
      "p, iterations = laplace2d_SOR_blitz(p, y, dx, dy, eps, omega)\n",
      "t2 = time.time()\n",
      "\n",
      "print \"Blitzed SOR method took {:.4f} seconds and {} iterations\\\n",
      " at tolerance {} with omega = {:.4f}\".format(t2 - t1, iterations, eps, omega)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wow!  That's *very* fast.  Also, $\\omega$ is very close to the upper limit of 2.  SOR tends to work fastest when $\\omega$ approaches 2, but don't be tempted to push it.  Set $\\omega = 2$ and the walls will come crumbling down.  \n",
      "\n",
      "How does the L2 error look with only 1324 iterations?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L2_rel_error(p,pan)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking very good, indeed.  \n",
      "\n",
      "We didn't explain it in any detail, but notice the very interesting implication of Equation $(5)$: the ideal relaxation factor is a function of the grid size.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scheduled Relaxation Jacobi (SRJ)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Scheduled Relaxation Jacobi method is the newest addition to the realm of iterative solver methods by a long shot.  In fact, it was published just last year, in June of 2014 in the [Journal of Computational Physics](http://www.sciencedirect.com/science/article/pii/S0021999114004173).  There's also a preprint of the paper [here](http://engineering.jhu.edu/fsag/wp-content/uploads/sites/23/2013/10/JCP_revised_WebPost.pdf) that isn't behind a paywall.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SRJ works in much the same way as the SOR method, but where SOR used a single tuned value of $\\omega$, SRJ uses a schedule of over- and under-relaxations.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each SRJ scheme is characterized by the number of levels $P$ which are the number of separate over- or under-relaxation parameters used ($\\omega_i$) with"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\overrightarrow{\\Omega} &= \\{\\omega_1, \\omega_2, ..., \\omega_p\\}\\\\\n",
      "\\overrightarrow{Q} &= \\{q_1, q_2, ..., q_p\\}\\\\\n",
      "\\end{align}\n",
      "\n",
      "where $q_i$ is the number of times the value $\\omega_i$ is repeated in one SRJ cycle, which consists of M total iterations where \n",
      "$$M = \\sum_{i=1}^p q_i$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SRJ scheduling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In theory, the order sequencing of various $\\omega$s doesn't matter, but in practice, roundoff error and arithmetic overflow cause issues.  In order to prevent instabilities, the relaxation parameters within a given SRJ cycle are (more or less) evenly distributed.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The larger the number of relaxation factors, $P$, the better the performance of the SRJ method.  Yang and Mittal provide precalculated relaxation factors up to $P=5$ which is what we will use.  \n",
      "\n",
      "For grid dimensions of 128 by 128, Yang and Mittal provide\n",
      "\n",
      "\\begin{align}\n",
      "\\Omega &= \\{4522.0, 580.86, 50.729, 4.5018, 0.67161\\}\\\\\n",
      "Q &= \\{1, 3, 16, 73, 250 \\}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The relaxation factor 4522.0 will be applied once, 580.86 will be applied three times... you get the idea.  \n",
      "\n",
      "We used a Python adapted version of code provided by Yang and Mittal to generate a variety of relaxation schedules (you can check out the code [here](https://bitbucket.org/cfdlab/jacobi/overview)).  \n",
      "\n",
      "Let's begin by loading the schedule file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "schedule = numpy.load('./data/relaxation_schedules.npz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can view the available schedules by asking for a list of files, e.g."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "schedule.files.sort()\n",
      "print schedule.files"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that each schedule listed is a NumPy array.  The number following `nx` is the grid dimension and the number follow `p` is the quantity of relaxation factors. \n",
      "\n",
      "The dimensions of the grid are $nx = ny = 128$ and we want to use $P = 5$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx128p5 = schedule['nx128p5']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The schedule is 343 elements long, so we don't need to print the whole array out, but we can take a look at the first 20 elements or so to see how the various relaxation factors are distributed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print nx128p5[:21]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "One last Neumann adventure"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The eagle-eyed among you may have noticed that in the SOR code above, we used the 2nd-order Neumann boundary condition that we initially derived for the traditional Jacobi method.  \n",
      "\n",
      "That is, despite the fact the the SOR scheme is discretized according to \n",
      "\n",
      "\\begin{equation}\n",
      "p^{n+1}_{i,j} = (1 - \\omega)p^n_{i,j} + \\frac{\\omega}{4} \\left(p^{n+1}_{i,j-1} + p^n_{i,j+1} + p^{n+1}_{i-1,j} + p^n_{i+1,j} \\right)\n",
      "\\end{equation}\n",
      "\n",
      "we used a Neumann condition based on the discretization\n",
      "\n",
      "\\begin{equation}\n",
      "p^{n+1}_{i,j} = \\frac{1}{4} \\left(p^{n}_{i,j-1} + p^n_{i,j+1} + p^{n}_{i-1,j} + p^n_{i+1,j} \\right)\n",
      "\\end{equation}\n",
      "\n",
      "and with an L2 relative error of $7.78613\\tt{E}$$^-5$, it doesn't seem to have caused any trouble.  \n",
      "\n",
      "*However*, SRJ uses **very** large relaxation factors.  Remember, we said that $\\omega = 2$ was unstable for SOR (and it is), but SRJ gets away with a relaxation factor of $\\omega = 4522$ (albeit for only one iteration).  \n",
      "\n",
      "The result is that we *do* need a more tailor-made Neumann condition for SRJ if we want to hang on to 2nd-order accuracy (and we do!).   \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The SRJ discretization is given by \n",
      "\n",
      "\\begin{equation}\n",
      "p^{n+1}_{i,j} = (1 - \\omega)p^n_{i,j} + \\frac{\\omega}{4} \\left(p^{n}_{i,j-1} + p^n_{i,j+1} + p^{n}_{i-1,j} + p^n_{i+1,j} \\right)\n",
      "\\end{equation}\n",
      "\n",
      "Using the same procedure as in [Lesson 1](./2D_Laplace_Equation.ipynb) on Equation $(52)$, we get the SRJ-friendly 2nd-order Neumann boundary condition:\n",
      "\n",
      "\\begin{equation}\n",
      "p^{n+1}_{0,j} = (1 - \\omega)p^n_{0,j} + \\frac{\\omega}{4} \\left(2p^{n}_{1,j} + p^{n}_{0,j-1} + p^n_{0,j+1}\\right)\n",
      "\\end{equation}\n",
      "\n",
      "It's probably a good idea for you to get out some pencil and paper and verify that."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SRJ with NumPy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok! We're finally here!  The SRJ function should be similar to what we've already done, except for a few key differences:\n",
      "\n",
      "*  There are no immediate updates, so we can use NumPy again!\n",
      "*  We cycle through the SRJ schedule in order -- then, if the relative L2 target hasn't been met, we cycle through again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def laplace2d_SRJ_numpy(p, y, dx, dy, l2_target, w_schedule):\n",
      "    '''Solves the diffusion equation with forward-time, centered scheme\n",
      "    \n",
      "    Parameters:\n",
      "    ----------\n",
      "    p: 2D array of float\n",
      "        Initial potential distribution\n",
      "    y: array of float\n",
      "        Nodal coordinates in y\n",
      "    dx: float\n",
      "        Mesh size\n",
      "    dy: float\n",
      "        Mesh size\n",
      "    l2_target: float\n",
      "        Error target\n",
      "    w_schedule: array of float\n",
      "        Relaxation schedule\n",
      "        \n",
      "    Returns:\n",
      "    -------\n",
      "    p: 2D array of float\n",
      "        Potential distribution after relaxation\n",
      "    '''\n",
      "    nx = len(y)\n",
      "        \n",
      "    l2norm = 1\n",
      "    pn = numpy.empty_like(p)\n",
      "    iteration_count = 0\n",
      "    \n",
      "    \n",
      "    while l2norm > l2_target:\n",
      "        for omega in w_schedule:\n",
      "            pn = p.copy()\n",
      "            p[1:-1,1:-1] = (1 - omega) * pn[1:-1,1:-1] + .25 * omega * (pn[1:-1,2:]+pn[1:-1,:-2]+pn[2:,1:-1]+pn[:-2,1:-1])\n",
      "                       \n",
      "            p[1:-1,-1] = (1 - omega) * pn[1:-1,-1] + .25 * omega *\\\n",
      "                        (2*pn[1:-1,-2]+pn[2:,-1]+pn[:-2,-1])\n",
      "            \n",
      "            l2norm = numpy.sqrt(numpy.sum((p - pn)**2)/numpy.sum(pn**2))\n",
      "            \n",
      "            if l2norm < l2_target:\n",
      "                break\n",
      "            iteration_count += 1\n",
      "     \n",
      "    return p, iteration_count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, let's try it out! "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = p0.copy()\n",
      "t1 = time.time()\n",
      "\n",
      "eps = 1e-8\n",
      "p, iterations = laplace2d_SRJ_numpy(p, y, dx, dy, eps, nx128p5)\n",
      "\n",
      "t2 = time.time()\n",
      "\n",
      "print \"SRJ method took {:.4f} seconds and {} iterations\\\n",
      " at tolerance {}\".format(t2 - t1, iterations, eps)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's definitely faster than Jacobi and Gauss-Seidel, but it's slower than SOR.  What gives?\n",
      "\n",
      "It's true that SOR is faster than SRJ... on one processor.  That last detail is what makes SRJ so compelling.  \n",
      "\n",
      "Remember, we had to do all sorts of code acrobatics with `weave` to get Gauss-Seidel and SOR to run quickly because they use immediately updated values, but SRJ doesn't.  SRJ is just a *hair* slower than SOR and we implemented it in straight NumPy.  \n",
      "\n",
      "We haven't talked much about parallelization in this course, but it's at the heart of most computational science.  \n",
      "\n",
      "Without getting into a detailed explanation, suffice it to say that SOR is *very* difficult to parallelize while SRJ is relatively trivial.  On one core, SOR is almost twice as fast as SRJ, but if we have four cores, then SRJ rules the roost.  \n",
      "\n",
      "If we have 8 cores, then SOR starts looking a little embarrassed.  \n",
      "\n",
      "And if we have a cluster with hundreds of cores?  You get the idea.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.  Yang, Xiyang IA, and Rajat Mittal. \"Acceleration of the Jacobi iterative method by factors exceeding 100 using scheduled relaxation.\" Journal of Computational Physics 274 (2014): 695-708.\n",
      "\n",
      "2.  Thomas, James William. Numerical partial differential equations: conservation laws and elliptic equations. Vol. 3. Berlin: Springer, 1999.\n",
      "\n",
      "3.  http://wiki.scipy.org/PerformancePython\n",
      "\n",
      "4.  http://www.physics.buffalo.edu/phy410-505/2011/topic3/app1/index.html\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "css_file = '../../styles/numericalmoocstyle.css'\n",
      "HTML(open(css_file, \"r\").read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}