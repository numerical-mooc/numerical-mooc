{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under MIT license © 2015 L.A. Barba, G.F. Forsyth, B. Knaepen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relax and hold steady"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fourth and last notebook of **Module 5** (*\"Relax and hold steady\"*), dedicated to elliptic PDEs. In the [previous notebook](./05_03_Iterate.This.ipynb), we examined how different algebraic formulations can speed up the iterative solution of the Laplace equation, compared to the simplest (but slowest) Jacobi method.  The Gauss-Seidel and successive-over relaxation methods both provide faster algebraic convergence than Jacobi. But there is still room for improvement.  \n",
    "\n",
    "In this lesson, we'll take a look at the very popular [conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method) (CG) method.  \n",
    "The CG method solves linear systems with coefficients matrices that are symmetric and positive-definite. It is either used on its own, or in conjunction with multigrid—a technique that we'll explore later on its own (optional) course module.\n",
    "\n",
    "For a real understanding of the CG method, there is no better option than studying the now-classic monograph by Jonathan Shewchuck: *\"An introduction to the conjugate gradient method withouth the agonizing pain\"* (1994). Here, we try to give you a brief summary to explain the implementation in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head in the right direction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the Poisson equation example from [Lesson 2](./05_02_2D.Poisson.Equation.ipynb).\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla^2 p = -2\\left(\\frac{\\pi}{2}\\right)^2\\sin\\left( \\frac{\\pi x}{L} \\right) \\cos\\left(\\frac{\\pi y}{L}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "in the domain \n",
    "\n",
    "$$\\left\\lbrace \\begin{align*}\n",
    "0 &\\leq x\\leq 1  \\\\\n",
    "-0.5 &\\leq y \\leq 0.5 \n",
    "\\end{align*} \\right.$$\n",
    "\n",
    "with boundary conditions \n",
    "\n",
    "$$p=0 \\text{ at } \\left\\lbrace \n",
    "\\begin{align*}\n",
    "x&=0\\\\\n",
    "y&=0\\\\\n",
    "y&=-0.5\\\\\n",
    "y&=0.5\n",
    "\\end{align*} \\right.$$\n",
    "\n",
    "We will solve this equation by assuming an initial state of $p=0$ everywhere, and applying boundary conditions to relax in pseudo-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CG method aims to solve elliptic PDEs by iterating the unknown and having it converge, up to a given accuracy, to the solution dictated by the boundary conditions (it is a member of the so-called Krylov subspace methods, but this should not scare you...).\n",
    "\n",
    "Recall that in its discretized form, the Poisson equation reads,\n",
    "\n",
    "$$\\frac{p_{i+1,j}^{k}-2p_{i,j}^{k}+p_{i-1,j}^{k}}{\\Delta x^2}+\\frac{p_{i,j+1}^{k}-2 p_{i,j}^{k}+p_{i,j-1}^{k}}{\\Delta y^2}=b_{i,j}^{k}$$\n",
    "\n",
    "You have manipulated equations of this kind in the past. Indeed, the left hand side represents a linear combination of the values of $p$ at several grid points and this linear combination has to be equal to the value of the source term, $b$, on the right hand side.\n",
    "\n",
    "Now imagine you gather the values $p_{i,j}$ of $p$ at all grid points into a big vector ${\\bf p}$ and you do the same for $b$ using the same ordering. Both vectors ${\\bf p}$ and ${\\bf b}$ contain $N=nx*ny$ values and thus belong to $\\mathbf{R}^N$. The linear relationship we just described means that the discretised Poisson equation can be symbolically written as\n",
    "\n",
    "\\begin{equation}\n",
    "A{\\bf p}={\\bf b}\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is a $N\\times N$ matrix. This is nothing other than a linear system of equations in matrix form. Although we will not directly use this form in the actual CG algorithm below, it is useful to examine the problem this way to understand how the method works.\n",
    "\n",
    "The starting point is to define the iteration of the potential $p$ as\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf p}^{k+1}={\\bf p}^k + \\alpha {\\bf d}^k\n",
    "\\end{equation}\n",
    "\n",
    "At iteration $n+1$, ${\\bf p}^k$ is modified by taking a step in the direction of ${\\bf d}^k$. The idea is to start with a guess ${\\bf p}^0$ and to march towards the solution by making jumps along the direction vectors ${\\bf d}^k$.  $\\alpha$ is a scalar that determines how big a jump to take at each iteration.  Now we need to carefully choose the direction vectors and the size of the jumps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the tools we use to determine the direction vectors is called the residual.  What is the residual?  We're glad you asked!\n",
    "\n",
    "If we assume that a solution exists to the Poisson equation\n",
    "\n",
    "\\begin{equation}\n",
    "A \\bf p = b\n",
    "\\end{equation}\n",
    "\n",
    "then we know that at each point in the domain, there will be some error between the calculated value, $p^k_i$ and the exact solution $p^{exact}_i$.  We may not know what the exact solution is, but we know it's out there.  The error at any point $i$ is the difference between the calculated value and the exact solution:\n",
    "\n",
    "\\begin{equation}\n",
    "e^k_i = p^k_i - p^{exact}_i\n",
    "\\end{equation}\n",
    "\n",
    "**Note:** We are talking about error at a specific point, not a measure of error across the entire domain.  \n",
    "\n",
    "What if we recast the Poisson equation in terms of a not-perfectly-relaxed $\\bf p^k$?\n",
    "\n",
    "\\begin{equation}\n",
    "A \\bf p^k \\approx b\n",
    "\\end{equation}\n",
    "\n",
    "We write this as an approximation because $\\bf p^k \\neq p$.  To \"fix\" the equation, we need to add an extra term to account for the difference in the Poisson equation $-$ that extra term is called the residual.  We can write out the modified Poisson equation like this:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf r^k} + A \\bf p^k = b\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The method of steepest descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before considering the more complex CG algorithm, it is helpful to introduce a simpler approach called the method of steepest descent. At iteration $0$, we choose an inital guess. Unless we are immensely lucky, it will not satisfy the Poisson equation and we will have,\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf b}-A{\\bf p}^0={\\bf r}^0\\ne {\\bf 0}\n",
    "\\end{equation}\n",
    "\n",
    "The vector ${\\bf r}^0$ is called the initial residual and measures how far we are from satisfying the linear system. In fact, we can define such a residual vector at each iteration,\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf r}^k={\\bf b}-A{\\bf p}^k\n",
    "\\end{equation}\n",
    "\n",
    "and monitor how this vector tends to zero as we iterate.\n",
    "\n",
    "In the method of steepest descent, two choices are made:\n",
    "\n",
    "1. the direction vectors are chosen to be the residuals ${\\bf d}^k = {\\bf r}^k$.\n",
    "2. the length of the jumps are such that the $n+1^{th}$ residual is orthogonal to the $n^{th}$ resuidual.\n",
    "\n",
    "<br>\n",
    "<div class=\"alert alert-success\">\n",
    "**Note:** A consequence of choice 1 is that the direction of jump $n+1$ is orthogonal to the direction of jump $n$.\n",
    "</div>\n",
    "\n",
    "There are good (and, in fact, not complex) reasons to justify these choices and you should read one of the references supplied if you want to understand them. But since we want you to converge to the end of the notebook in a finite time, please accept them for now. \n",
    "\n",
    "Condition 2 requires that,\n",
    "\n",
    "\\begin{align}\n",
    "{\\bf r}^{k+1}\\cdot {\\bf r}^{k} = 0 \\nonumber \\\\\n",
    "\\Leftrightarrow ({\\bf b}-A{\\bf p}^{k+1}) \\cdot {\\bf r}^{k} = 0 \\nonumber \\\\\n",
    "\\Leftrightarrow ({\\bf b}-A({\\bf p}^{k}+\\alpha {\\bf r}^k)) \\cdot {\\bf r}^{k} = 0 \\nonumber \\\\\n",
    "\\Leftrightarrow ({\\bf r}^k+\\alpha A{\\bf r}^k) \\cdot {\\bf r}^{k} = 0 \\nonumber \\\\\n",
    "\\alpha = \\frac{{\\bf r}^k \\cdot {\\bf r}^k}{A{\\bf r}^k \\cdot {\\bf r}^k}.\n",
    "\\end{align}\n",
    "\n",
    "We are now ready to test this algorithm.\n",
    "\n",
    "To begin, let's import libraries, setup our mesh and import some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from math import pi\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from laplace_helper import plot_3D, L2_rel_error\n",
    "from cg_helper import poisson_2d, p_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nx = 101\n",
    "ny = 101\n",
    "xmin = 0\n",
    "xmax = 1\n",
    "ymin = -0.5\n",
    "ymax = 0.5\n",
    "\n",
    "l2_target = 1e-10\n",
    "\n",
    "# Spacing\n",
    "dx = (xmax-xmin)/(nx-1)\n",
    "dy = (ymax-ymin)/(ny-1)\n",
    "\n",
    "# Mesh\n",
    "x  = numpy.linspace(xmin,xmax,nx)\n",
    "y  = numpy.linspace(ymin,ymax,ny)\n",
    "X,Y = numpy.meshgrid(x,y)\n",
    "\n",
    "# Source\n",
    "L = xmax-xmin\n",
    "b = -2*(pi/L)**2*numpy.sin(pi*X/L)*numpy.cos(pi*Y/L)\n",
    "\n",
    "# Initialization\n",
    "p_i  = numpy.zeros((ny,nx))\n",
    "\n",
    "# Analytical solution\n",
    "pan = p_analytical(X,Y,L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to implement steepest descent!  \n",
    "\n",
    "Let's quickly review the solution process:\n",
    "\n",
    "1. Calculate the residual, $\\bf r^k$, which also serves as the direction vector, $\\bf d^k$\n",
    "2. Calculate the step size $\\alpha$\n",
    "3. Update ${\\bf p}^{k+1}={\\bf p}^k + \\alpha {\\bf d}^k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do we calculate the residual?  \n",
    "\n",
    "We have an equation for the residual above: \n",
    "\\begin{equation}\n",
    "{\\bf r}^k={\\bf b}-A{\\bf p}^k\n",
    "\\end{equation}\n",
    "\n",
    "Remember that $A$ is just a stand-in for the Laplacian $\\nabla^2$ and since $dx=dy$ we know that the Laplacian can be discretized as\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla^2 p^k = 4p^k_{i,j} - \\left(p^{k}_{i,j-1} + p^k_{i,j+1} + p^{k}_{i-1,j} + p^k_{i+1,j} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "##### What about calculating $\\alpha$?\n",
    "\n",
    "The calculation of $\\alpha$ is relatively straightforward, but does require evaluating the term $A{\\bf r^k}$.  That said, we just went over how to handle discretizing the $A$ operator.\n",
    "\n",
    "Given that, you shouldn't have too much trouble writing out the discretization for $A{\\bf r^k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def steepest_descent_2d(p, b, dx, dy, l2_target):\n",
    "    '''\n",
    "    Performs steepest descent relaxation\n",
    "    Assumes Dirichlet boundary conditions p=0\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    p : 2D array of floats\n",
    "        Initial guess\n",
    "    b : 2D array of floats\n",
    "        Source term\n",
    "    dx: float\n",
    "        Mesh spacing in x direction\n",
    "    dy: float\n",
    "        Mesh spacing in y direction\n",
    "    l2_target: float\n",
    "        Error target\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    p: 2D array of float\n",
    "        Distribution after relaxation\n",
    "    '''\n",
    "    r  = numpy.zeros((ny,nx)) # residual\n",
    "    Ar  = numpy.zeros((ny,nx)) # to store result of matrix multiplication\n",
    "    \n",
    "    l2_norm = 1\n",
    "    iterations = 0\n",
    "    l2_conv = []\n",
    "    \n",
    "    # Iterations\n",
    "    while l2_norm > l2_target:\n",
    "\n",
    "        pd = p.copy()\n",
    "        \n",
    "        r[1:-1,1:-1] = b[1:-1,1:-1]*dx**2 + 4*pd[1:-1,1:-1] - \\\n",
    "            pd[1:-1,2:] - pd[1:-1,:-2] - pd[2:, 1:-1] - pd[:-2, 1:-1]\n",
    "        \n",
    "        Ar[1:-1,1:-1] = -4*r[1:-1,1:-1]+r[1:-1,2:]+r[1:-1,:-2]+\\\n",
    "            r[2:, 1:-1] + r[:-2, 1:-1]\n",
    "\n",
    "        rho = numpy.sum(r*r)\n",
    "        sigma = numpy.sum(r*Ar)\n",
    "        alpha = rho/sigma\n",
    "\n",
    "        p = pd + alpha*r\n",
    "        \n",
    "        # BCs are automatically enforced\n",
    "        \n",
    "        l2_norm = L2_rel_error(pd,p)\n",
    "        iterations += 1\n",
    "        l2_conv.append(l2_norm)\n",
    "    \n",
    "    print('Number of SD iterations: {0:d}'.format(iterations))\n",
    "    return p, l2_conv     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs on our example problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, l2_conv = steepest_descent_2d(p_i.copy(), b, dx, dy, l2_target)\n",
    "L2_rel_error(p, pan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Not bad! it took only two iterations to reach a solution that meets our steady state check. Although this seems great, the steepest descent algorithm is not great when used with large systems or more complex right hand sides in the Poisson equation (we'll examine this below!). We can get better performance if we take a little more care in selecting the direction vectors, $\\bf d^k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The method of conjugate gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With steepest descent, we know that two **successive** jumps are orthogonal, but that's about it.  There is nothing to prevent the algorithm from making several jumps in the same (or a similar) direction.  Imagine you wanted to go from the intersection of 5th Avenue and 23rd Street to the intersection of 9th Avenue and 30th Street. Knowing that each segment has the same computational cost (one iteration), would you follow the red path or the green path?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/jumps.png\" width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1. Do you take the red path or the green path?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\\bf r}^{k+1} \\cdot {\\bf r}^{k+1": {}
    }
   },
   "source": [
    "The method of conjugate gradients reduces the number of jumps by making sure the algorithm never selects the same direction twice. The size of the jumps is now given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha = \\frac{{\\bf r}^k \\cdot {\\bf r}^k}{A{\\bf d}^k \\cdot {\\bf d}^k}\n",
    "\\end{equation}\n",
    "\n",
    "and the direction vectors by:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf d}^{k+1}={\\bf r}^{k+1}+\\beta{\\bf d}^{k}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta = \\frac{{\\bf r}^{k+1} \\cdot {\\bf r}^{k+1}}{{\\bf r}^k \\cdot {\\bf r}^k}$.\n",
    "\n",
    "The search directions are no longer equal to the residuals but are instead a  linear combination of the residual and the previous search direction. It can be shown that this algorithm necessarily converges to the exact solution (up to machine accuracy) in a maximum of $N$ iterations. When one is satisfied with an approximate solution, the number of steps is significantly lower. Again, the derivation of the algorithm is not immensely difficult and can be found in Shewchuk (1994).\n",
    "\n",
    "Here is a function implementing the CG algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Conjugate Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\\bf r}^{k+1} \\cdot {\\bf r}^{k+1": {}
    }
   },
   "source": [
    "We will again update $\\bf p$ according to \n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf p}^{k+1}={\\bf p}^k + \\alpha {\\bf d}^k\n",
    "\\end{equation}\n",
    "\n",
    "but use the modified equations above to calculate $\\alpha$ and ${\\bf d}^k$.  \n",
    "\n",
    "You may have noticed that $\\beta$ depends on both ${\\bf r}^{k+1}$ and ${\\bf r}^k$ and that makes the calculation of ${\\bf d}^0$ a little bit tricky.  Or impossible (using the formula above).  Instead we set ${\\bf d}^0 = {\\bf r}^0$ for the first step and then switch for all subsequent iterations.  \n",
    "\n",
    "Thus, the full set of steps for the method of conjugate gradients is:\n",
    "\n",
    "Calculate ${\\bf d}^0 = {\\bf r}^0$ (just the once), then\n",
    "\n",
    "1. Calculate $\\alpha = \\frac{{\\bf r}^k \\cdot {\\bf r}^k}{A{\\bf d}^k \\cdot {\\bf d}^k}$\n",
    "2. Update ${\\bf p}^{k+1}$\n",
    "3. Calculate ${\\bf r}^{k+1} = {\\bf r}^k - \\alpha A {\\bf d}^k$ $\\ \\ \\ \\ $(see <a href='#references'>Shewchuk (1994)</a>)\n",
    "4. Calculate $\\beta = \\frac{{\\bf r}^{k+1} \\cdot {\\bf r}^{k+1}}{{\\bf r}^k \\cdot {\\bf r}^k}$\n",
    "5. Calculate ${\\bf d}^{k+1}={\\bf r}^{k+1}+\\beta{\\bf d}^{k}$\n",
    "6. Repeat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conjugate_gradient_2d(p, b, dx, dy, l2_target):\n",
    "    '''Performs cg relaxation\n",
    "    Assumes Dirichlet boundary conditions p=0\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    p : 2D array of floats\n",
    "        Initial guess\n",
    "    b : 2D array of floats\n",
    "        Source term\n",
    "    dx: float\n",
    "        Mesh spacing in x direction\n",
    "    dy: float\n",
    "        Mesh spacing in y direction\n",
    "    l2_target: float\n",
    "        Error target\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    p: 2D array of float\n",
    "        Distribution after relaxation\n",
    "    '''\n",
    "    r  = numpy.zeros((ny,nx)) # residual\n",
    "    Ad  = numpy.zeros((ny,nx)) # to store result of matrix multiplication \n",
    "    \n",
    "    l2_norm = 1\n",
    "    iterations = 0\n",
    "    l2_conv = []\n",
    "    \n",
    "    # Step-0 We compute the initial residual and \n",
    "    # the first search direction is just this residual\n",
    "    \n",
    "    r[1:-1,1:-1] = b[1:-1,1:-1]*dx**2 + 4*p[1:-1,1:-1] - \\\n",
    "        p[1:-1,2:] - p[1:-1,:-2] - p[2:, 1:-1] - p[:-2, 1:-1]\n",
    "    d = r.copy()\n",
    "    rho = numpy.sum(r*r)\n",
    "    Ad[1:-1,1:-1] = -4*d[1:-1,1:-1]+d[1:-1,2:]+d[1:-1,:-2]+\\\n",
    "        d[2:, 1:-1] + d[:-2, 1:-1]\n",
    "    sigma = numpy.sum(d*Ad)\n",
    "    \n",
    "    # Iterations\n",
    "    while l2_norm > l2_target:\n",
    "\n",
    "        pk = p.copy()\n",
    "        rk = r.copy()\n",
    "        dk = d.copy()\n",
    "        \n",
    "        alpha = rho/sigma\n",
    "\n",
    "        p = pk + alpha*dk\n",
    "        r = rk- alpha*Ad\n",
    "        \n",
    "        rhop1 = numpy.sum(r*r)\n",
    "        beta = rhop1 / rho\n",
    "        rho = rhop1\n",
    "        \n",
    "        d = r + beta*dk\n",
    "        Ad[1:-1,1:-1] = -4*d[1:-1,1:-1] + d[1:-1,2:] + d[1:-1,:-2] + \\\n",
    "            d[2:, 1:-1] + d[:-2, 1:-1]\n",
    "        sigma = numpy.sum(d*Ad)\n",
    "        \n",
    "        # BCs are automatically enforced\n",
    "        \n",
    "        l2_norm = L2_rel_error(pk,p)\n",
    "        iterations += 1\n",
    "        l2_conv.append(l2_norm)\n",
    "    \n",
    "    print('Number of CG iterations: {0:d}'.format(iterations))\n",
    "    return p, l2_conv     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, l2_conv = conjugate_gradient_2d(p_i.copy(), b, dx, dy, l2_target)\n",
    "L2_rel_error(p,pan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of conjugate gradients also took two iterations to reach a solution that meets our steady state check.\n",
    "\n",
    "Let's compare this to the number of iterations needed for the Jacobi iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, l2_conv = poisson_2d(p_i.copy(), b, dx, dy, l2_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the problem we considered, we observed a significant gain in terms of computational cost using the method of steepest descent or the conjugate gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Poisson problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conjugate gradient method really shines when one needs to solve more complex Poisson problems. To get an insight into this, let's solve the Poisson problem using the same boundary conditions as the previous problem but with the following right hand side,\n",
    "\n",
    "\\begin{equation}\n",
    "b = \\sin\\left(\\frac{\\pi x}{L}\\right) \\cos\\left(\\frac{\\pi y}{L}\\right) + \\sin\\left(\\frac{6\\pi x}{L}\\right) \\cos\\left(\\frac{6\\pi y}{L}\\right)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = (numpy.sin(pi*X/L)*numpy.cos(pi*Y/L)+\n",
    "     numpy.sin(6*pi*X/L)*numpy.sin(6*pi*Y/L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, l2_conv = poisson_2d(p_i.copy(), b, dx, dy, l2_target)\n",
    "\n",
    "p, l2_conv = steepest_descent_2d(p_i.copy(), b, dx, dy, l2_target)\n",
    "\n",
    "p, l2_conv = conjugate_gradient_2d(p_i.copy(), b, dx, dy, l2_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='references'></a>\n",
    "Shewchuk, J. (1994). [An Introduction to the Conjugate Gradient Method Without the Agonizing Pain (PDF)](http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)\n",
    "\n",
    "Ilya Kuzovkin, [The Concept of Conjugate Gradient Descent in Python](http://ikuz.eu/2015/04/15/the-concept-of-conjugate-gradient-descent-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "css_file = '../../styles/numericalmoocstyle.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
